# LDD 方法论

> 人机协作的迭代优化：人类定标准，AI 执行循环

```
传统：Human-in-the-loop（人类执行）
LDD：Human-on-the-loop（人类定标准 + 监督，AI 执行）
```

## 设计原理

### 核心洞察

**AI 的能力边界**：
- ✅ 擅长：执行、迭代、模式匹配、结构化输出
- ❌ 不擅长：判断"什么是好的"、主观质量评估

**LDD 的定位**：
- 不是"让 AI 自己优化自己"
- 而是"让人类定义标准，AI 执行优化循环"

### 三个核心假设

| 假设 | 定义 | 失败时 |
|------|------|--------|
| **可观测性** | 优化目标的质量可以被结构化捕获 | 无法形成反馈循环 |
| **可验证性** | 存在**外部锚定**的判断方式 | 无法判断"更好"或"更差" |
| **可修改性** | AI 能根据反馈调整目标 | 无法迭代 |

**关键修正**：可验证性需要"外部锚定"，纯 AI 评估不算有效验证。

---

## 核心循环

```
E → O → V → M → (Loop)

Execute：执行可观测动作
Observe：捕获结构化结果
Verify：判断是否达标（需要外部锚定）
Modify：根据反馈修改目标
```

---

## 验证强度模型

LDD 区分三种验证强度：

| 强度 | 来源 | 例子 | 可靠性 |
|------|------|------|--------|
| **强验证** | 硬指标 | 测试通过、Schema 校验、断言 | 高 |
| **强验证** | 外部信号 | 用户反馈、线上指标、A/B 测试 | 高 |
| **中验证** | 人类抽检 | 定期人工审核 + AI 辅助筛选 | 中 |
| **弱验证** | 纯 AI 评估 | AI 评分、AI 对比 | 低 |

### 弱验证的问题

```
问题：用 AI 评估 AI
  - 如果 AI 能准确判断"什么是好的"
  - 它为什么不能直接生成"好的"输出？
  - 这是一个逻辑矛盾

结论：弱验证只能用于红旗检测，不能作为优化依据
```

---

## 三层验证（修订版）

| 层级 | 角色 | 输出 | 说明 |
|------|------|------|------|
| **L1** | 硬验证 | Pass/Fail | 断言、正则、Schema、测试 |
| **L2** | 红旗检测 | Flags[] | AI 标记明显问题，**不输出分数** |
| **L3** | 人类校准 | Score + Feedback | 锚定样本 + 定期抽检 |

### L2 的重新定位

**旧版 L2**（有问题）：
```json
{ "score": 7.4, "feedback": "..." }
```

**新版 L2**（红旗检测）：
```json
{
  "flags": ["missing_suggestion", "vague_message"],
  "severity": "warning" | "critical",
  "needsHumanReview": true,
  "details": "消息过于模糊，缺少解决建议"
}
```

**设计原则**：
- L2 不假装能精确评分
- 只标记"明显的问题"（高置信度的坏）
- 不确定的情况推给人类

### 综合验证公式

```
旧版：Score = L1×W1 + L2×W2 + L3×W3 （有问题）

新版：
  L1_pass = 所有硬验证通过
  L2_flags = AI 检测到的红旗数量
  L3_score = 人类评分（来自抽检或锚定样本）

  Pass = L1_pass AND L2_flags == 0 AND L3_score ≥ 阈值

  如果 L3 不可用（无人类评分）：
    Pass = L1_pass AND L2_flags == 0
    ⚠️ 标记为"弱验证通过"，需要人类确认
```

---

## 人类参与模型

### 参与时机

| 阶段 | 人类职责 | 工具支持 |
|------|----------|----------|
| **设计时** | 定义验证标准、提供锚定样本 | AskUserQuestion |
| **运行时** | 定期抽检、处理红旗 | 抽检提醒 |
| **异常时** | 审核报告、决策是否继续 | 暂停机制 |

### 抽检频率

根据验证强度自动调整：

| 验证强度 | 抽检频率 |
|----------|----------|
| 强（硬指标 + 外部信号） | 每 10 轮 |
| 中（有锚定样本） | 每 5 轮 |
| 弱（纯 AI 评估） | 每轮 |

### 暂停触发条件

| 触发 | 行为 |
|------|------|
| L2 检测到 critical 红旗 | 立即暂停 |
| 连续 2 次 L3 评分下降 | 暂停，回滚最佳版本 |
| 震荡（A→B→A） | 暂停，请求人类决策 |
| 抽检未通过 | 暂停，审核修改方向 |

---

## 适用边界

| ✅ 适合 | ❌ 不适合 |
|---------|----------|
| 有硬指标的任务 | 纯主观任务 |
| 有外部信号的任务 | 无法量化的任务 |
| 人类愿意定期参与 | 期望全自动化 |
| 可迭代改进 | 单次决策 |

**判断标准**：能否找到外部锚定的验证方式？

---

## 已知局限

| 局限 | 说明 | 缓解 |
|------|------|------|
| L2 无法精确评估主观质量 | 这是 AI 的能力边界 | 降级为红旗检测 |
| 人类参与成本 | 需要定期抽检 | 验证强度越高，抽检越少 |
| 锚定样本覆盖有限 | 3-5 个样本无法覆盖所有情况 | 持续补充 |
| 局部最优 | 可能陷入局部最优 | 人类审核修改方向 |

---

## 文件结构

```
.ldd/{project}/
├─ LOOP-DESIGN.md     # Loop 设计文档
├─ target.*           # 被优化对象
├─ executor.*         # 执行器
├─ verifier.*         # 验证器（L1 + L2 红旗检测）
├─ anchors/           # 锚定样本（人类标注）
├─ reviews/           # 人类抽检记录
├─ flags.json         # L2 红旗历史
└─ changes.json       # 修改记录
```

---

## 元验证：如何判断 Loop 设计是否有效

| 标准 | 检验方式 |
|------|----------|
| **收敛性** | 分数在 N 轮内趋于稳定或达标 |
| **可重复性** | 相同输入多次执行，结果稳定 |
| **区分度** | L2 能标记出坏样本，不误伤好样本 |
| **人类一致性** | L3 抽检结果与锚定样本一致 |

如果不满足这些标准，问题可能在：
- 验证标准定义不清（需要人类重新定义）
- L2 红旗规则过松/过严
- 优化目标本身不适合 LDD

---

## 版本历史

### v2.0（当前）
- 重新定义可验证性假设：需要外部锚定
- L2 从"评分器"改为"红旗检测器"
- 增加验证强度模型
- 增加人类参与模型
- 移除不可靠的加权评分公式

### v1.0
- 初始版本
- 问题：过度依赖 AI 评估

---

*LDD 的价值不在于全自动化，而在于结构化人机协作。*
